{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93d41384",
   "metadata": {},
   "source": [
    "#\n",
    "<h1 style=\"text-align: center;\">Sentiment Analysis RELOADED</h1>\n",
    "\n",
    "<h4 style=\"text-align: center;\">\n",
    "\n",
    "Esteban Gomez Valerio\n",
    "\n",
    "Roi Jared Flores Garza Stone\n",
    "\n",
    "Rafael Takata Garcia\n",
    "\n",
    "Text Mining - O2025_MAF3654H\n",
    "\n",
    "Ing. Juan Antonio Vega Fernández, M. Sc., M. T. Ed\n",
    "\n",
    "ITESO\n",
    "</h4>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3ab84",
   "metadata": {},
   "source": [
    "### Context:\n",
    "In the modern political landscape, social media platforms have transformed into an indispensable barometer of public opinion and a driving force in shaping electoral discourse. The X platform (formerly Twitter), in particular, is an epicenter where candidates, media, and voters interact in real-time, generating a massive flow of textual data that reflects the collective mood.\n",
    "\n",
    "The 2024 U.S. Presidential election cycle is an event of global significance, and the ability to measure, understand, and predict public sentiment via social media is crucial for political analysts, campaign strategists, and academics.\n",
    "\n",
    "### Objective: \n",
    "This Text Analysis Project aims to decode this digital pulse by applying advanced Natural Language Processing (NLP) and Machine Learning techniques.\n",
    "\n",
    "We will focus on analyzing the Kaggle dataset titled [2024 U.S. Election Sentiment on X](https://www.kaggle.com/datasets/emirhanai/2024-u-s-election-sentiment-on-x/data?select=train.csv) which provides an labeled corpus of posts capturing the conversation around key candidates and political parties.\n",
    "\n",
    "### Goals\n",
    "\n",
    "- Classify and quantify the sentiment (positive, negative, neutral) of posts directed at the main candidates and parties.\n",
    "\n",
    "- Identify patterns and trends in the polarization of the discourse over time.\n",
    "\n",
    "- Extract key topics that dominate the online electoral conversation.\n",
    "\n",
    "### Structure \n",
    "To achieve these objectives, the project will be based on the analysis of the train.csv file from the dataset. This dataset is a robust source containing the post text, pre-existing sentiment labels, and rich metadata on party affiliation and engagement metrics (likes, retweets).\n",
    "\n",
    "The methodology will include the following key steps:\n",
    "\n",
    "Text Preprocessing: Data cleaning, tokenization, and language normalization.\n",
    "\n",
    "Exploratory Data Analysis (EDA): Visualization of sentiment distribution and most frequent words.\n",
    "\n",
    "Sentiment Modeling: Training a classification model (e.g., based on Transformers or traditional Machine Learning) to automatically predict and validate sentiment.\n",
    "\n",
    "The final outcome will be a deep, data-driven understanding of the emotional and thematic dynamics shaping the 2024 presidential race in the digital sphere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ee1c15",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "12c6b775",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import Counter\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b699c",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f98feca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>user_handle</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>candidate</th>\n",
       "      <th>party</th>\n",
       "      <th>retweets</th>\n",
       "      <th>likes</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>@user123</td>\n",
       "      <td>11/3/2024 8:45</td>\n",
       "      <td>Excited to see Kamala Harris leading the Democ...</td>\n",
       "      <td>Kamala Harris</td>\n",
       "      <td>Democratic Party</td>\n",
       "      <td>120</td>\n",
       "      <td>450</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>@politicsFan</td>\n",
       "      <td>11/3/2024 9:15</td>\n",
       "      <td>Donald Trump's policies are the best for our e...</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>Republican Party</td>\n",
       "      <td>85</td>\n",
       "      <td>300</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>@greenAdvocate</td>\n",
       "      <td>11/3/2024 10:05</td>\n",
       "      <td>Jill Stein's environmental plans are exactly w...</td>\n",
       "      <td>Jill Stein</td>\n",
       "      <td>Green Party</td>\n",
       "      <td>60</td>\n",
       "      <td>200</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>@indieVoice</td>\n",
       "      <td>11/3/2024 11:20</td>\n",
       "      <td>Robert Kennedy offers a fresh perspective outs...</td>\n",
       "      <td>Robert Kennedy</td>\n",
       "      <td>Independent</td>\n",
       "      <td>40</td>\n",
       "      <td>150</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>@libertyLover</td>\n",
       "      <td>11/3/2024 12:35</td>\n",
       "      <td>Chase Oliver's libertarian stance promotes tru...</td>\n",
       "      <td>Chase Oliver</td>\n",
       "      <td>Libertarian Party</td>\n",
       "      <td>30</td>\n",
       "      <td>120</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   tweet_id     user_handle        timestamp  \\\n",
       "0         1        @user123   11/3/2024 8:45   \n",
       "1         2    @politicsFan   11/3/2024 9:15   \n",
       "2         3  @greenAdvocate  11/3/2024 10:05   \n",
       "3         4     @indieVoice  11/3/2024 11:20   \n",
       "4         5   @libertyLover  11/3/2024 12:35   \n",
       "\n",
       "                                          tweet_text       candidate  \\\n",
       "0  Excited to see Kamala Harris leading the Democ...   Kamala Harris   \n",
       "1  Donald Trump's policies are the best for our e...    Donald Trump   \n",
       "2  Jill Stein's environmental plans are exactly w...      Jill Stein   \n",
       "3  Robert Kennedy offers a fresh perspective outs...  Robert Kennedy   \n",
       "4  Chase Oliver's libertarian stance promotes tru...    Chase Oliver   \n",
       "\n",
       "               party  retweets  likes sentiment  \n",
       "0   Democratic Party       120    450  positive  \n",
       "1   Republican Party        85    300  positive  \n",
       "2        Green Party        60    200  positive  \n",
       "3        Independent        40    150   neutral  \n",
       "4  Libertarian Party        30    120  positive  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"../data/train.csv\")\n",
    "df_test = pd.read_csv(\"../data/test.csv\")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d797e99",
   "metadata": {},
   "source": [
    "In this case, we will only need the `Democratic` & `Republican` party in order to make them a conflict of just two factions, plus we will need only the text of the tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "064dc1d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dataset: 1000\n",
      "\n",
      "Sentiment Distribution:\n",
      "sentiment\n",
      "positive    650\n",
      "neutral     240\n",
      "negative    110\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "X_train = df_train['tweet_text']\n",
    "y_train = df_train['sentiment']\n",
    "\n",
    "X_test = df_test['tweet_text']\n",
    "y_test = df_test['sentiment']\n",
    "\n",
    "print(f\"Length of the dataset: {len(df_train)}\")\n",
    "print(\"\\nSentiment Distribution:\")\n",
    "print(df_train['sentiment'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79fefa7d",
   "metadata": {},
   "source": [
    "### Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "063feb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(\n",
    "        stop_words='english',\n",
    "        ngram_range=(1, 2)\n",
    "    )),\n",
    "    \n",
    "    ('lr', LogisticRegression(\n",
    "        random_state=42,\n",
    "        solver='liblinear'\n",
    "    ))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cc6667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ITESOO\\Mineria_textos\\Sentiment_Analysis_Reloaded\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1296: FutureWarning: Using the 'liblinear' solver for multiclass classification is deprecated. An error will be raised in 1.8. Either use another solver which supports the multinomial loss or wrap the estimator in a OneVsRestClassifier to keep applying a one-versus-rest scheme.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# training the baseline model on the entire dataset\n",
    "baseline_pipeline.fit(X_train, y_train)\n",
    "y_pred = baseline_pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fb8d9a44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the Baseline Model: 0.8636\n",
      "--------------------------------------------------\n",
      "Informe de Clasificación:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.83      0.29      0.43        17\n",
      "     neutral       1.00      0.93      0.96        29\n",
      "    positive       0.82      0.98      0.89        64\n",
      "\n",
      "    accuracy                           0.86       110\n",
      "   macro avg       0.88      0.74      0.76       110\n",
      "weighted avg       0.87      0.86      0.84       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy of the Baseline Model: {accuracy:.4f}\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"Informe de Clasificación:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffb40a12",
   "metadata": {},
   "source": [
    "This baseline Logistic Regression model achieves a decent overall Accuracy of 86.36%. The model performs well on the `Neutral class` (perfect Precision and Recall), indicating these samples are easily separable. Performance on the `Positive class` is also strong (F1-score of 0.89), due to its high Recall (0.98), meaning it successfully identifies almost all positive tweets. However, the model presents a critical weakness in identifying the `Negative class`: while its Precision is reasonable (0.83), its Recall is extremely low (0.26)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a15f77f",
   "metadata": {},
   "source": [
    "### Feature Engineering with POS and NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "38a39ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b091b686",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Model ready\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "print(\" Model ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38d458b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "POS_TAGS = ['NOUN', 'VERB', 'ADJ', 'ADV', 'PRON', 'DET', 'ADP', 'AUX', 'SCONJ', 'CCONJ', 'INTJ']\n",
    "NER_TAGS = ['PERSON', 'ORG', 'GPE', 'LOC', 'DATE', 'TIME', 'NORP', 'EVENT']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99e56f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pos_ner_counts(text):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "    features = Counter()\n",
    "    \n",
    "    # 1. Count of POS (Part of Speech) Tags\n",
    "    for token in doc:\n",
    "        if token.pos_ in POS_TAGS:\n",
    "            features[f\"POS_{token.pos_}\"] += 1\n",
    "            \n",
    "    # 2. Count of NED (Named Entity Recognition)\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_ in NER_TAGS:\n",
    "            features[f\"NER_{ent.label_}\"] += 1\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1e95bf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tweet: Excited to see Kamala Harris leading the Democratic charge!\n",
      "Counts: Counter({'POS_ADJ': 2, 'POS_VERB': 2, 'POS_DET': 1, 'POS_NOUN': 1, 'NER_PERSON': 1, 'NER_NORP': 1})\n"
     ]
    }
   ],
   "source": [
    "# Prove the function with a sample \n",
    "sample_tweet = X_train.iloc[0]\n",
    "sample_features = extract_pos_ner_counts(sample_tweet)\n",
    "print(f\"Tweet: {sample_tweet}\")\n",
    "print(f\"Counts: {sample_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7919882a",
   "metadata": {},
   "source": [
    "| Tag | Meaning | Sentiment Relevance |\n",
    "| :--- | :--- | :--- |\n",
    "| **POS\\_ADJ** | Adjective | Describe something (e.g., \"terrible,\" \"excellent\"). |\n",
    "| **POS\\_VERB** | Verb | Defines the action (e.g., \"supports,\" \"criticizes\"). |\n",
    "| **NER\\_PERSON** | Named Person | Identifies the subject (e.g., a candidate). |\n",
    "| **NER\\_ORG** | Organization | Identifies the associated group or political party. |\n",
    "\n",
    "### Examples in our output\n",
    "\n",
    "The test example (`Tweet: Excited to see Kamala Harris leading the Democratic charge!`) validates that the function correctly maps the language to numerical features:\n",
    "\n",
    "* **`POS_ADJ: 2`**: \"Excited,\" \"Democratic\"\n",
    "* **`NER_PERSON: 1`**: \"Kamala Harris\"\n",
    "* **`POS_VERB: 2`**: \"see,\" \"leading\"\n",
    "* **`POS_DET: 1`**: \"the\"\n",
    "* **`POS_NOUN: 1`**: \"charge\"\n",
    "* **`NER_NORP: 1`**: \"Democratic\"\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a759d42d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting POS/NER features for Training Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b810ed2711bc4dd4bc386e028e037a40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete for Training Set.\n",
      "Extracting POS/NER features for Test Set...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad8748c1156e4a11a13527cd665d4244",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/110 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extraction complete for Test Set.\n",
      "\n",
      "Training Features Shape: (1000, 15)\n",
      "Test Features Shape: (110, 15)\n",
      "Number of unique POS/NER features extracted: 15\n"
     ]
    }
   ],
   "source": [
    "# 1. Training Data\n",
    "print(\"Extracting POS/NER features for Training Set...\")\n",
    "X_train_dicts = [extract_pos_ner_counts(text) for text in tqdm(X_train)]\n",
    "print(\"Extraction complete for Training Set.\")\n",
    "\n",
    "# 2. Vectorizer fitting\n",
    "vectorizer = DictVectorizer(dtype=float) \n",
    "X_train_features = vectorizer.fit_transform(X_train_dicts)\n",
    "\n",
    "# 3. Feature Extraction on Test Data\n",
    "print(\"Extracting POS/NER features for Test Set...\")\n",
    "X_test_dicts = [extract_pos_ner_counts(text) for text in tqdm(X_test)]\n",
    "print(\"Extraction complete for Test Set.\")\n",
    "\n",
    "# 4. Vectorizer transformation \n",
    "X_test_features = vectorizer.transform(X_test_dicts)\n",
    "\n",
    "print(f\"\\nTraining Features Shape: {X_train_features.shape}\")\n",
    "print(f\"Test Features Shape: {X_test_features.shape}\")\n",
    "print(f\"Number of unique POS/NER features extracted: {X_train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "596f2e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP...\n",
      "Training complete.\n",
      "------------------------------------------------------------\n",
      "Accuracy of the POS/NER Model (MLP): 0.7545\n",
      "------------------------------------------------------------\n",
      "Classification Report (MLP on POS/NER Features):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.35      0.46        17\n",
      "     neutral       0.74      0.79      0.77        29\n",
      "    positive       0.77      0.84      0.81        64\n",
      "\n",
      "    accuracy                           0.75       110\n",
      "   macro avg       0.73      0.66      0.68       110\n",
      "weighted avg       0.75      0.75      0.74       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Matrix conversion\n",
    "X_train_dense = X_train_features.toarray().astype('float64')\n",
    "X_test_dense = X_test_features.toarray().astype('float64')\n",
    "\n",
    "# Filter classes \n",
    "class_counts = y_train.value_counts()\n",
    "valid_classes = class_counts[class_counts >= 2].index\n",
    "\n",
    "# Filter Training Set\n",
    "train_mask = y_train.isin(valid_classes)\n",
    "X_train_filtered = X_train_dense[train_mask.values, :] \n",
    "y_train_filtered = y_train.loc[train_mask]\n",
    "\n",
    "# Filter Test Set\n",
    "test_mask = y_test.isin(valid_classes)\n",
    "X_test_filtered = X_test_dense[test_mask.values, :]\n",
    "y_test_filtered = y_test.loc[test_mask]\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_train_encoded = le.fit_transform(y_train_filtered.values)\n",
    "y_test_encoded = le.transform(y_test_filtered.values)\n",
    "\n",
    "X_train_final = X_train_filtered\n",
    "X_test_final = X_test_filtered\n",
    "y_train_final = y_train_encoded\n",
    "y_test_final = y_test_encoded\n",
    "\n",
    "# MLP Model Definition \n",
    "mlp_model = MLPClassifier(\n",
    "    hidden_layer_sizes=(100, 50),\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    solver='adam', \n",
    "    early_stopping=True \n",
    ")\n",
    "\n",
    "# Training\n",
    "print(\"\\nTraining MLP...\")\n",
    "mlp_model.fit(X_train_final, y_train_final) \n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Predictions and Evaluation\n",
    "y_pred_nn = mlp_model.predict(X_test_final)\n",
    "\n",
    "y_pred_decoded = le.inverse_transform(y_pred_nn)\n",
    "y_test_decoded = le.inverse_transform(y_test_final)\n",
    "\n",
    "accuracy_nn = accuracy_score(y_test_decoded, y_pred_decoded)\n",
    "\n",
    "# Results \n",
    "print(\"-\" * 60)\n",
    "print(f\"Accuracy of the POS/NER Model (MLP): {accuracy_nn:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Classification Report (MLP on POS/NER Features):\")\n",
    "print(classification_report(y_test_decoded, y_pred_decoded, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d9ef0c",
   "metadata": {},
   "source": [
    "The MLP model, relying only on absatract linguistic counts (like the number of adjectives or verbs) rather than the actual words themselves, resulted in a lower overall accuracy ($\\mathbf{0.74}$) compared to the inflated Logistic Regression baseline ($\\mathbf{0.86}$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15068b5",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6da3fda",
   "metadata": {},
   "source": [
    "### Transformer-Based Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c838e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'distilbert-base-uncased'\n",
    "\n",
    "device = torch.device('cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f478371d",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {id: name for id, name in enumerate(le.classes_)}\n",
    "num_labels = len(le.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cacff32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data ready for Trainer. Number of labels: 3\n",
      "Label mapping (ID to Name): {0: 'negative', 1: 'neutral', 2: 'positive'}\n"
     ]
    }
   ],
   "source": [
    "# Tokenization of Texts\n",
    "train_encodings = tokenizer(list(X_train.values), truncation=True, padding=True)\n",
    "test_encodings = tokenizer(list(X_test.values), truncation=True, padding=True)\n",
    "\n",
    "\n",
    "# PyTorch Dataset Class Definition\n",
    "class SentimentDataset(Dataset):\n",
    "    def __init__(self, encodings, labels):\n",
    "        self.encodings = encodings\n",
    "        self.labels = labels\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Return token IDs, attention mask, and the integer label\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['labels'] = torch.tensor(self.labels[idx])\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "# Create Dataset instances\n",
    "train_dataset = SentimentDataset(train_encodings, y_train_encoded)\n",
    "test_dataset = SentimentDataset(test_encodings, y_test_encoded)\n",
    "\n",
    "print(\"Data ready for Trainer. Number of labels:\", num_labels)\n",
    "print(\"Label mapping (ID to Name):\", label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a5ad6e08",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting manual Fine-Tuning on CPU for 3 epochs...\n",
      "--- Epoch 1/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bfeb263b04e4884a558227cd22fe2da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 2/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2ba2eb86d4240acadf3b7131b50c913",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Epoch 3/3 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ada42fa4eef476f95d8fc05d98b2fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# 1. Load the Model and Move to CPU\n",
    "model = DistilBertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=num_labels)\n",
    "# CRITICAL: Model moved to CPU\n",
    "model.to(device) \n",
    "model.train() # Set model to training mode\n",
    "\n",
    "# 2. Define Optimizer and Learning Rate\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# 3. Define Parameters\n",
    "epochs = 3\n",
    "# IMPORTANT: Increase batch size slightly for CPU, but keep it modest (e.g., 32 or 64) if memory allows\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True) \n",
    "\n",
    "# 4. Training Loop\n",
    "print(f\"\\nStarting manual Fine-Tuning on CPU for {epochs} epochs...\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"--- Epoch {epoch + 1}/{epochs} ---\")\n",
    "    \n",
    "    # Training loop by batch\n",
    "    for batch in tqdm(train_loader):\n",
    "        optimizer.zero_grad() # Clear gradients\n",
    "        \n",
    "        # CRITICAL: Move data batches to CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Pass data to the model\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n",
    "        loss = outputs.loss\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4989949",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c0896cde9b354ed2a55ab0aec904b2c4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Accuracy: 0.8636\n",
      "------------------------------------------------------------\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.67      0.35      0.46        17\n",
      "     neutral       0.97      1.00      0.98        29\n",
      "    positive       0.85      0.94      0.89        64\n",
      "\n",
      "    accuracy                           0.86       110\n",
      "   macro avg       0.83      0.76      0.78       110\n",
      "weighted avg       0.85      0.86      0.85       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Final Evaluation \n",
    "model.eval() \n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=64) \n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        # Move data to CPU\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        # Get the predicted class\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Decode and Final Results\n",
    "y_pred_final = le.inverse_transform(np.array(all_preds))\n",
    "y_test_final = le.inverse_transform(np.array(all_labels))\n",
    "\n",
    "accuracy_transformer = accuracy_score(y_test_final, y_pred_final)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Accuracy: {accuracy_transformer:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test_final, y_pred_final, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bf4614",
   "metadata": {},
   "source": [
    "The DistilBERT model delivered strong overall results with a robust accuracy of 85.45%. It was perfect at identifying the major sentiment groups: `Neutral` tweets were classified perfectly (Recall 1.00), and the `Positive` tweets were almost always found (Recall 0.97). However, the model ran into the exact same fundamental problem as the simpler Baseline: it failed  on the hard-to-find Negative tweets. The model could only correctly identify a mere 26% of the actual `negative` examples, confirming that the high ambiguity or noise in that minority class remains a critical weakness, even for an advanced Transformer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ed9643",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be42823d",
   "metadata": {},
   "source": [
    "### Mixed Embeddings + Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "278bc32d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS Vocabulary Size: 15\n",
      "Shape of padded POS ID array (Train): torch.Size([1000, 128])\n"
     ]
    }
   ],
   "source": [
    "# Load spaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "MAX_LEN = 128 \n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "def get_pos_sequence(text):\n",
    "    \"\"\"Processes text to return a sequence of POS tags (strings).\"\"\"\n",
    "    doc = nlp(text)\n",
    "    pos_tags = [token.pos_ for token in doc]\n",
    "    return pos_tags\n",
    "\n",
    "# Get POS sequences for training and testing\n",
    "X_train_pos_sequences = [get_pos_sequence(text) for text in X_train]\n",
    "X_test_pos_sequences = [get_pos_sequence(text) for text in X_test]\n",
    "\n",
    "# Build the vocabulary of all unique POS tags\n",
    "all_pos_tags = set(tag for seq in X_train_pos_sequences for tag in seq)\n",
    "pos_vocab = {tag: i + 1 for i, tag in enumerate(sorted(list(all_pos_tags)))}\n",
    "pos_vocab['<PAD>'] = 0 # Padding token ID\n",
    "POS_VOCAB_SIZE = len(pos_vocab)\n",
    "\n",
    "print(f\"POS Vocabulary Size: {POS_VOCAB_SIZE}\")\n",
    "\n",
    "# Function to convert POS sequence strings to IDs, and pad them\n",
    "def encode_and_pad_pos(pos_sequences, pos_vocab, max_len=MAX_LEN):\n",
    "    encoded_sequences = []\n",
    "    for seq in pos_sequences:\n",
    "        # Convert tags to IDs, use 0 for padding\n",
    "        ids = [pos_vocab.get(tag, pos_vocab['<PAD>']) for tag in seq]\n",
    "        \n",
    "        # Pad or truncate to max_len\n",
    "        if len(ids) < max_len:\n",
    "            ids.extend([pos_vocab['<PAD>']] * (max_len - len(ids)))\n",
    "        else:\n",
    "            ids = ids[:max_len]\n",
    "            \n",
    "        encoded_sequences.append(torch.tensor(ids, dtype=torch.long))\n",
    "        \n",
    "    return torch.stack(encoded_sequences)\n",
    "\n",
    "# Encode and pad all sequences\n",
    "train_pos_ids = encode_and_pad_pos(X_train_pos_sequences, pos_vocab)\n",
    "test_pos_ids = encode_and_pad_pos(X_test_pos_sequences, pos_vocab)\n",
    "\n",
    "print(f\"Shape of padded POS ID array (Train): {train_pos_ids.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "51a59ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridDataset(Dataset):\n",
    "    def __init__(self, encodings, pos_ids, labels):\n",
    "        self.encodings = encodings\n",
    "        self.pos_ids = pos_ids\n",
    "        self.labels = np.array(labels, dtype=np.int64)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n",
    "        item['pos_ids'] = self.pos_ids[idx]\n",
    "        item['labels'] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "        \n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "train_dataset_hybrid = HybridDataset(train_encodings, train_pos_ids, y_train_encoded)\n",
    "test_dataset_hybrid = HybridDataset(test_encodings, test_pos_ids, y_test_encoded)\n",
    "\n",
    "class HybridBiLSTM(nn.Module):\n",
    "    def __init__(self, pos_vocab_size, word_vocab_size, pos_embedding_dim=50, \n",
    "                 word_embedding_dim=768, hidden_dim=128, num_labels=3, dropout=0.3):\n",
    "        super(HybridBiLSTM, self).__init__()\n",
    "        \n",
    "        self.word_vocab_size = word_vocab_size \n",
    "        self.word_embedding_dim = word_embedding_dim\n",
    "        \n",
    "        # Embedding Layer para POS\n",
    "        self.pos_embedding = nn.Embedding(\n",
    "            num_embeddings=pos_vocab_size, \n",
    "            embedding_dim=pos_embedding_dim, \n",
    "            padding_idx=0 \n",
    "        )\n",
    "        \n",
    "        # Embedding Layer para Palabras\n",
    "        self.word_embedding = nn.Embedding(self.word_vocab_size, self.word_embedding_dim)\n",
    "\n",
    "        total_input_dim = self.word_embedding_dim + pos_embedding_dim\n",
    "        \n",
    "        # BiLSTM Layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=total_input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Classification Head\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, num_labels)\n",
    "        \n",
    "    def forward(self, input_ids, pos_ids):\n",
    "        # Word Embedding (SIMULACIÓN)\n",
    "        word_embedded = self.word_embedding(input_ids)\n",
    "        \n",
    "        current_seq_len = input_ids.shape[1] \n",
    "\n",
    "        # POS Embedding\n",
    "        pos_embedded = self.pos_embedding(pos_ids) \n",
    "        \n",
    "        # Corrección de longitud (del paso anterior)\n",
    "        if pos_embedded.shape[1] > current_seq_len:\n",
    "            pos_embedded = pos_embedded[:, :current_seq_len, :]\n",
    "        elif pos_embedded.shape[1] < current_seq_len:\n",
    "             word_embedded = word_embedded[:, :pos_embedded.shape[1], :]\n",
    "\n",
    "\n",
    "        # Concatenación\n",
    "        combined_sequence = torch.cat((word_embedded, pos_embedded), dim=2) \n",
    "        \n",
    "        # BiLSTM Forward Pass\n",
    "        lstm_out, _ = self.lstm(combined_sequence)\n",
    "        \n",
    "        # Global Average Pooling (GAP)\n",
    "        pooled_output = torch.mean(lstm_out, dim=1) \n",
    "        \n",
    "        # Classification\n",
    "        dropped_output = self.dropout(pooled_output)\n",
    "        logits = self.classifier(dropped_output)\n",
    "        self.labels = np.array(labels, dtype=np.int64)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "56aebce1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Hybrid BiLSTM Training on cpu for 3 epochs...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab70ce6ba72f4a488fdc6ad8258c5f10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\esteb\\AppData\\Local\\Temp\\ipykernel_29232\\2316982660.py:81: DeprecationWarning: __array__ implementation doesn't accept a copy keyword, so passing copy=False failed. __array__ must implement 'dtype' and 'copy' keyword arguments. To learn more, see the migration guide https://numpy.org/devdocs/numpy_2_0_migration_guide.html#adapting-to-changes-in-the-copy-keyword\n",
      "  self.labels = np.array(labels, dtype=np.int64)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Loss: 0.9476\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51cc19a0b9614bf18d9fddd532b7df9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 Loss: 0.8362\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e2bf8ab279c4dcdbed884161ec7d692",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Loss: 0.8065\n",
      "\n",
      "✅ Hybrid Training complete.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec44b654ac845fb9d1733f0c49b618a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluation:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------\n",
      "Accuracy of the Hybrid BiLSTM Model: 0.5909\n",
      "------------------------------------------------------------\n",
      "Classification Report (Hybrid BiLSTM):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.00      0.00      0.00        17\n",
      "     neutral       1.00      0.03      0.07        29\n",
      "    positive       0.59      1.00      0.74        64\n",
      "\n",
      "    accuracy                           0.59       110\n",
      "   macro avg       0.53      0.34      0.27       110\n",
      "weighted avg       0.61      0.59      0.45       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Params of the model\n",
    "WORD_VOCAB_SIZE = 30000  \n",
    "POS_EMBEDDING_DIM = 50 \n",
    "HIDDEN_DIM = 128\n",
    "NUM_EPOCHS = 3\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Model\n",
    "model_hybrid = HybridBiLSTM(\n",
    "    pos_vocab_size=POS_VOCAB_SIZE, \n",
    "    word_vocab_size=WORD_VOCAB_SIZE,\n",
    "    word_embedding_dim=768,\n",
    "    pos_embedding_dim=POS_EMBEDDING_DIM, \n",
    "    hidden_dim=HIDDEN_DIM, \n",
    "    num_labels=num_labels\n",
    ")\n",
    "model_hybrid.to(device)\n",
    "\n",
    "# DataLoader\n",
    "train_loader_hybrid = DataLoader(train_dataset_hybrid, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader_hybrid = DataLoader(test_dataset_hybrid, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Función de Pérdida y Optimizador\n",
    "optimizer = AdamW(model_hybrid.parameters(), lr=5e-5)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training\n",
    "print(f\"\\nStarting Hybrid BiLSTM Training on {device} for {NUM_EPOCHS} epochs...\")\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model_hybrid.train() \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in tqdm(train_loader_hybrid, desc=f\"Epoch {epoch+1}\"):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pos_ids = batch['pos_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        logits = model_hybrid(input_ids, pos_ids)\n",
    "        \n",
    "        loss = loss_fn(logits, labels)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch {epoch+1} Loss: {total_loss / len(train_loader_hybrid):.4f}\")\n",
    "\n",
    "print(\"\\n✅ Hybrid Training complete.\")\n",
    "\n",
    "\n",
    "# 5. Evaluación Final\n",
    "model_hybrid.eval() \n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader_hybrid, desc=\"Evaluation\"):\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        pos_ids = batch['pos_ids'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "\n",
    "        logits = model_hybrid(input_ids, pos_ids)\n",
    "        preds = torch.argmax(logits, dim=-1)\n",
    "        \n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "\n",
    "y_pred_final_hybrid = le.inverse_transform(np.array(all_preds))\n",
    "y_test_final_hybrid = le.inverse_transform(np.array(all_labels))\n",
    "\n",
    "accuracy_hybrid = accuracy_score(y_test_final_hybrid, y_pred_final_hybrid)\n",
    "\n",
    "print(\"-\" * 60)\n",
    "print(f\"Accuracy of the Hybrid BiLSTM Model: {accuracy_hybrid:.4f}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print(\"Classification Report (Hybrid BiLSTM):\")\n",
    "print(classification_report(y_test_final_hybrid, y_pred_final_hybrid, zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33acc50d",
   "metadata": {},
   "source": [
    "The BiLSTM model failed to learn how to classify and, instead, found the path of least resistance: predicting the majority class ('`positive`') for absolutely everything. This is confirmed because its overall Accuracy (0.57) is identical to the support percentage of the 'positive' class in the dataset, and its Recall is 1.00 for that class but 0.00 for all others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a24c1",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
